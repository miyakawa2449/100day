{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed598fd788c54bdaa644fb4fc9982eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb6764e1800d436a99ce1f6cfd45b19c",
              "IPY_MODEL_3333093d57994cdb845d9fa244fd0f2a",
              "IPY_MODEL_bcd909e38f0541bf9402a1f7eb2f8a17"
            ],
            "layout": "IPY_MODEL_4716bf972f794fbe836d5b0420588f61"
          }
        },
        "fb6764e1800d436a99ce1f6cfd45b19c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c6a6f9502fc4e968f34d723c0afaf09",
            "placeholder": "​",
            "style": "IPY_MODEL_3cb179369fbf4c96bc2c9a52d54acce7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3333093d57994cdb845d9fa244fd0f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e9e4a89dbb8496295630f12b351d268",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59ccbb595fe04e36a77b9cbecf2158d3",
            "value": 4
          }
        },
        "bcd909e38f0541bf9402a1f7eb2f8a17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_011a51e134b14c24a673fde62c2f21d6",
            "placeholder": "​",
            "style": "IPY_MODEL_f2647eec5f43493ea9ea20d99a1826ee",
            "value": " 4/4 [00:12&lt;00:00,  2.58s/it]"
          }
        },
        "4716bf972f794fbe836d5b0420588f61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c6a6f9502fc4e968f34d723c0afaf09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb179369fbf4c96bc2c9a52d54acce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e9e4a89dbb8496295630f12b351d268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59ccbb595fe04e36a77b9cbecf2158d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "011a51e134b14c24a673fde62c2f21d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2647eec5f43493ea9ea20d99a1826ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# セル 1: 依存関係のインストール (バージョンを調整)\n",
        "!pip install flask pyngrok -U -qq\n",
        "\n",
        "# PyTorch と torchvision のバージョンを指定 (Colab GPU環境向け)\n",
        "# CUDA 11.8 環境 (T4 GPUなど) を想定\n",
        "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118 -U -qq\n",
        "# Colabの環境によっては CUDA 12.1 (cu121) が適切な場合もあります。!nvcc --version で確認できます。\n",
        "\n",
        "# Transformers と関連ライブラリ (sentence-transformers と互換性のあるバージョンを指定)\n",
        "# sentence-transformers 3.4.1 は transformers>=4.41.0 を要求するため、4.41.2 を試す\n",
        "!pip install transformers==4.41.2 accelerate sentencepiece google-colab -U -qq # <- ここを変更"
      ],
      "metadata": {
        "id": "RFYphcnXCMgr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import torch\n",
        "from flask import Flask, request, jsonify, render_template_string # render_template_stringに変更\n",
        "from pyngrok import ngrok, conf, exception as pyngrok_exception\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig # 量子化を使う場合\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "# --- 設定項目 ---\n",
        "MODEL_NAME = \"rinna/llama-3-youko-8b\"\n",
        "# MODEL_NAME = \"stabilityai/japanese-stablelm-2-1_6b\" # 必要に応じて軽量なモデルに変更\n",
        "MODEL_DIR = \"/content/drive/MyDrive/llama_model_cache\" # Google Drive内のキャッシュディレクトリ名（任意）\n",
        "NGROK_SECRET_NAME = \"NGROK_AUTHTOKEN\" # ステップ1で設定したColabシークレット名\n",
        "FLASK_PORT = 5000 # Flaskが使用するポート\n",
        "USE_GOOGLE_DRIVE_CACHE = True # Google Driveをキャッシュとして使うか\n",
        "# --- ここまで ---\n",
        "\n",
        "# セル 2: インポートと基本設定 の logging 設定部分を修正\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, # INFOレベル以上を出力\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "    force=True # Colab環境で設定を確実に上書き\n",
        ")\n",
        "# pyngrokのロガーもINFOに設定（何が起きているか少し詳しく見るため）\n",
        "logging.getLogger(\"pyngrok\").setLevel(logging.INFO)\n",
        "\n",
        "logger = logging.getLogger(__name__) # このモジュール用のロガーを取得\n",
        "\n",
        "# グローバル変数 (初期化)\n",
        "tokenizer = None\n",
        "model = None\n",
        "ngrok_tunnel = None # ngrokトンネルオブジェクト"
      ],
      "metadata": {
        "id": "4Mue6Q64HjEl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_GOOGLE_DRIVE_CACHE:\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        logger.info(\"Google Drive mounted successfully at /content/drive.\")\n",
        "        # キャッシュディレクトリが存在しない場合は作成\n",
        "        cache_path = os.path.join(MODEL_DIR)\n",
        "        os.makedirs(cache_path, exist_ok=True)\n",
        "        logger.info(f\"Model cache directory set to: {cache_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to mount Google Drive: {e}\", exc_info=True)\n",
        "        logger.warning(\"Proceeding without Google Drive model caching.\")\n",
        "        MODEL_DIR = None # Driveが使えない場合はキャッシュ無効化\n",
        "else:\n",
        "    logger.info(\"Google Drive caching is disabled.\")\n",
        "    MODEL_DIR = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omiyWbYCHoVT",
        "outputId": "74eeffba-bdc4-43d1-f7b6-f2534153afa2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-03 03:50:11 - __main__ - INFO - Google Drive mounted successfully at /content/drive.\n",
            "2025-05-03 03:50:11 - __main__ - INFO - Model cache directory set to: /content/drive/MyDrive/llama_model_cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Colabシークレットからトークンを取得\n",
        "    ngrok_token = userdata.get(NGROK_SECRET_NAME)\n",
        "    if not ngrok_token:\n",
        "         raise ValueError(f\"'{NGROK_SECRET_NAME}' value is empty in Colab secrets.\")\n",
        "\n",
        "    # pyngrokのデフォルト設定に認証トークンをセット\n",
        "    conf.get_default().auth_token = ngrok_token\n",
        "    logger.info(\"Ngrok authentication token set successfully from Colab secrets.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    logger.error(f\"Fatal: Ngrok authtoken secret '{NGROK_SECRET_NAME}' not found in Colab secrets.\")\n",
        "    logger.error(\"Please follow Step 1 to add your ngrok authtoken as a secret.\")\n",
        "    # トークンがないとngrokは使えないのでエラーにする\n",
        "    raise SystemExit(\"Ngrok authtoken is required to proceed.\")\n",
        "except ValueError as e:\n",
        "    logger.error(f\"Fatal: {e}\")\n",
        "    raise SystemExit(\"Ngrok authtoken configuration error.\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Fatal: An unexpected error occurred while setting ngrok auth token: {e}\", exc_info=True)\n",
        "    raise SystemExit(\"Ngrok authtoken configuration error.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-ba0t0EHuRf",
        "outputId": "51448b96-3a3e-4830-d1ca-934d17b02d5b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-03 03:50:17 - __main__ - INFO - Ngrok authentication token set successfully from Colab secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model_and_tokenizer(model_name, cache_dir):\n",
        "    \"\"\"モデルとトークナイザーをロードまたはダウンロードする\"\"\"\n",
        "    global tokenizer, model\n",
        "    logger.info(f\"Preparing model and tokenizer for: {model_name}\")\n",
        "\n",
        "    # 量子化設定 (メモリ削減のため、必要に応じて有効化)\n",
        "    # bnb_config = BitsAndBytesConfig(\n",
        "    #     load_in_4bit=True,\n",
        "    #     bnb_4bit_use_double_quant=True,\n",
        "    #     bnb_4bit_quant_type=\"nf4\",\n",
        "    #     bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    # )\n",
        "\n",
        "    model_load_path = model_name # デフォルトはHuggingFace Hubから\n",
        "    load_from_cache = False\n",
        "\n",
        "    # Google Driveキャッシュが有効で、キャッシュが存在するかチェック\n",
        "    if cache_dir and os.path.exists(os.path.join(cache_dir, model_name.replace('/', '_'), \"config.json\")):\n",
        "        model_load_path = os.path.join(cache_dir, model_name.replace('/', '_'))\n",
        "        logger.info(f\"Found model cache in Google Drive: {model_load_path}\")\n",
        "        load_from_cache = True\n",
        "    elif cache_dir:\n",
        "        logger.info(f\"Model cache not found in {cache_dir}. Will download and potentially save.\")\n",
        "        # 保存先パスだけ設定\n",
        "        model_save_path = os.path.join(cache_dir, model_name.replace('/', '_'))\n",
        "    else:\n",
        "        logger.info(\"Google Drive cache is disabled or unavailable. Loading directly from HuggingFace Hub.\")\n",
        "        model_save_path = None # 保存しない\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Loading tokenizer from: {model_load_path}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_load_path)\n",
        "\n",
        "        logger.info(f\"Loading model from: {model_load_path}\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_load_path,\n",
        "            torch_dtype=torch.bfloat16, # BF16を使用 (対応GPUが必要)\n",
        "            device_map=\"auto\",          # 自動でデバイス (GPU/CPU) に割り当て\n",
        "            # quantization_config=bnb_config, # 量子化を使う場合コメント解除\n",
        "            low_cpu_mem_usage=True,     # CPUメモリ使用量を抑える試み\n",
        "        )\n",
        "        logger.info(\"Model and tokenizer loaded successfully.\")\n",
        "\n",
        "        # キャッシュからロードしなかった場合で、保存先が指定されていれば保存\n",
        "        if not load_from_cache and model_save_path:\n",
        "             try:\n",
        "                 logger.info(f\"Saving model and tokenizer to cache: {model_save_path}\")\n",
        "                 os.makedirs(model_save_path, exist_ok=True)\n",
        "                 tokenizer.save_pretrained(model_save_path)\n",
        "                 model.save_pretrained(model_save_path)\n",
        "                 logger.info(\"Successfully saved model and tokenizer to cache.\")\n",
        "             except Exception as save_e:\n",
        "                 logger.error(f\"Failed to save model/tokenizer to cache directory {model_save_path}: {save_e}\", exc_info=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Fatal error during model/tokenizer preparation: {e}\", exc_info=True)\n",
        "        if \"out of memory\" in str(e).lower():\n",
        "            logger.error(\">>> Hint: You might be running out of GPU memory. Try a smaller model or enable quantization.\")\n",
        "        raise SystemExit(\"Failed to prepare model and tokenizer.\")\n",
        "\n",
        "# 実行\n",
        "prepare_model_and_tokenizer(MODEL_NAME, MODEL_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "ed598fd788c54bdaa644fb4fc9982eb2",
            "fb6764e1800d436a99ce1f6cfd45b19c",
            "3333093d57994cdb845d9fa244fd0f2a",
            "bcd909e38f0541bf9402a1f7eb2f8a17",
            "4716bf972f794fbe836d5b0420588f61",
            "7c6a6f9502fc4e968f34d723c0afaf09",
            "3cb179369fbf4c96bc2c9a52d54acce7",
            "9e9e4a89dbb8496295630f12b351d268",
            "59ccbb595fe04e36a77b9cbecf2158d3",
            "011a51e134b14c24a673fde62c2f21d6",
            "f2647eec5f43493ea9ea20d99a1826ee"
          ]
        },
        "id": "n9jRyBz3Hweu",
        "outputId": "8dffd7a2-654a-4687-80a6-8a4910cb28d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-03 03:50:27 - __main__ - INFO - Preparing model and tokenizer for: rinna/llama-3-youko-8b\n",
            "2025-05-03 03:50:27 - __main__ - INFO - Found model cache in Google Drive: /content/drive/MyDrive/llama_model_cache/rinna_llama-3-youko-8b\n",
            "2025-05-03 03:50:27 - __main__ - INFO - Loading tokenizer from: /content/drive/MyDrive/llama_model_cache/rinna_llama-3-youko-8b\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "2025-05-03 03:50:28 - __main__ - INFO - Loading model from: /content/drive/MyDrive/llama_model_cache/rinna_llama-3-youko-8b\n",
            "2025-05-03 03:50:28 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed598fd788c54bdaa644fb4fc9982eb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-03 03:50:40 - __main__ - INFO - Model and tokenizer loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "# --- HTMLテンプレート ---\n",
        "# render_template_stringを使うため、HTMLをPython文字列として定義\n",
        "chat_html_template = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"ja\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Chatbot Interface</title>\n",
        "    <style>\n",
        "        body { font-family: sans-serif; margin: 0; padding: 0; background-color: #f0f2f5; display: flex; justify-content: center; align-items: center; min-height: 100vh; }\n",
        "        .chat-container { background-color: #fff; box-shadow: 0 4px 12px rgba(0,0,0,0.1); border-radius: 8px; width: 90%; max-width: 700px; height: 85vh; display: flex; flex-direction: column; overflow: hidden; }\n",
        "        h1 { text-align: center; color: #1c1e21; margin: 20px 0; font-size: 1.6em; font-weight: 600; }\n",
        "        #chat-box { flex-grow: 1; padding: 20px; overflow-y: auto; border-top: 1px solid #dddfe2; border-bottom: 1px solid #dddfe2; background-color: #fff; }\n",
        "        .message { margin-bottom: 15px; display: flex; flex-direction: column; max-width: 85%; word-wrap: break-word; }\n",
        "        .message span { padding: 10px 15px; border-radius: 18px; line-height: 1.4; font-size: 0.95em; }\n",
        "        .user { align-self: flex-end; }\n",
        "        .user span { background-color: #0084ff; color: white; border-bottom-right-radius: 5px; }\n",
        "        .bot { align-self: flex-start; }\n",
        "        .bot span { background-color: #e4e6eb; color: #050505; border-bottom-left-radius: 5px; }\n",
        "        .thinking span { background-color: #f0f0f0; color: #65676b; font-style: italic; }\n",
        "        .error span { background-color: #fdeded; color: #d32f2f; border: 1px solid #f5c6cb; }\n",
        "        .input-area { display: flex; padding: 15px; border-top: 1px solid #dddfe2; background-color: #f0f2f5; }\n",
        "        #user-input { flex-grow: 1; padding: 12px 18px; border: 1px solid #ccd0d5; border-radius: 20px; margin-right: 10px; font-size: 1em; outline: none; transition: border-color 0.2s ease; }\n",
        "        #user-input:focus { border-color: #0084ff; }\n",
        "        #send-button { padding: 12px 20px; background-color: #0084ff; color: white; border: none; border-radius: 20px; cursor: pointer; font-size: 1em; transition: background-color 0.2s ease; font-weight: 600; }\n",
        "        #send-button:hover:not(:disabled) { background-color: #0073e0; }\n",
        "        #send-button:disabled { background-color: #bcc0c4; cursor: not-allowed; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"chat-container\">\n",
        "        <h1>AI Chatbot</h1>\n",
        "        <div id=\"chat-box\">\n",
        "             <div class=\"message bot\"><span>こんにちは！メッセージを入力してください。</span></div>\n",
        "        </div>\n",
        "        <div class=\"input-area\">\n",
        "            <input type=\"text\" id=\"user-input\" placeholder=\"ここにメッセージを入力...\" autocomplete=\"off\">\n",
        "            <button id=\"send-button\" onclick=\"sendMessage()\">送信</button>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        const chatBox = document.getElementById('chat-box');\n",
        "        const userInput = document.getElementById('user-input');\n",
        "        const sendButton = document.getElementById('send-button');\n",
        "\n",
        "        // メッセージをチャットボックスに追加する関数\n",
        "        function addMessage(text, type) {\n",
        "            const messageDiv = document.createElement('div');\n",
        "            messageDiv.className = `message ${type}`;\n",
        "            // テキスト内の改行を<br>に変換して表示\n",
        "            const formattedText = text.replace(/\\\\n/g, '<br>');\n",
        "            messageDiv.innerHTML = `<span>${formattedText}</span>`;\n",
        "            chatBox.appendChild(messageDiv);\n",
        "            // 遅延させてからスクロールすることで、レンダリング後に確実に最下部に移動\n",
        "            setTimeout(() => { chatBox.scrollTop = chatBox.scrollHeight; }, 50);\n",
        "            return messageDiv;\n",
        "        }\n",
        "\n",
        "        // メッセージ送信処理\n",
        "        async function sendMessage() {\n",
        "            const messageText = userInput.value.trim();\n",
        "            if (!messageText) return; // 空のメッセージは無視\n",
        "\n",
        "            addMessage(messageText, 'user'); // ユーザーメッセージを表示\n",
        "            userInput.value = ''; // 入力欄をクリア\n",
        "            userInput.disabled = true; // 入力と送信を無効化\n",
        "            sendButton.disabled = true;\n",
        "\n",
        "            const thinkingMessage = addMessage(\"考え中...\", 'thinking bot'); // 思考中メッセージ\n",
        "\n",
        "            try {\n",
        "                // /chatエンドポイントにPOSTリクエスト\n",
        "                const response = await fetch(\"/chat\", {\n",
        "                    method: \"POST\",\n",
        "                    headers: { \"Content-Type\": \"application/json\" },\n",
        "                    body: JSON.stringify({ message: messageText })\n",
        "                });\n",
        "\n",
        "                chatBox.removeChild(thinkingMessage); // 思考中メッセージを削除\n",
        "\n",
        "                const data = await response.json(); // レスポンスをJSONとして解析\n",
        "\n",
        "                if (!response.ok) {\n",
        "                    // サーバーからのエラーレスポンス\n",
        "                    addMessage(`エラー: ${data.error || response.statusText || '不明なサーバーエラー'}`, 'error bot');\n",
        "                    console.error(\"Server error response:\", data);\n",
        "                } else {\n",
        "                    // 正常な応答\n",
        "                    addMessage(data.response || \"応答がありませんでした。\", 'bot');\n",
        "                }\n",
        "            } catch (error) {\n",
        "                // ネットワークエラーなど\n",
        "                chatBox.removeChild(thinkingMessage); // 思考中メッセージを削除\n",
        "                addMessage(\"通信エラーが発生しました。接続を確認してください。\", 'error bot');\n",
        "                console.error(\"Fetch API error:\", error);\n",
        "            } finally {\n",
        "                // 応答後に入力と送信を再度有効化\n",
        "                userInput.disabled = false;\n",
        "                sendButton.disabled = false;\n",
        "                userInput.focus(); // 入力欄にフォーカス\n",
        "                // 最終スクロール\n",
        "                setTimeout(() => { chatBox.scrollTop = chatBox.scrollHeight; }, 50);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Enterキーで送信\n",
        "        userInput.addEventListener('keypress', function(event) {\n",
        "            if (event.key === 'Enter' && !sendButton.disabled) {\n",
        "                sendMessage();\n",
        "            }\n",
        "        });\n",
        "\n",
        "        // ページ読み込み時に入力欄にフォーカス\n",
        "        userInput.focus();\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "# --- ここまでHTML ---\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    \"\"\"チャットUIのHTMLを表示するルート\"\"\"\n",
        "    # render_template_stringを使用してPython文字列内のHTMLをレンダリング\n",
        "    return render_template_string(chat_html_template)\n",
        "\n",
        "@app.route(\"/chat\", methods=[\"POST\"])\n",
        "def chat():\n",
        "    \"\"\"チャットリクエストを受け取り、モデルの応答を返すAPIエンドポイント\"\"\"\n",
        "    global tokenizer, model\n",
        "    # モデルがロードされているか確認\n",
        "    if not tokenizer or not model:\n",
        "         logger.error(\"Chat request received but model/tokenizer is not ready.\")\n",
        "         return jsonify({\"error\": \"モデルが準備できていません。管理者に連絡してください。\"}), 503 # Service Unavailable\n",
        "\n",
        "    try:\n",
        "        # リクエストボディからメッセージを取得\n",
        "        user_input = request.json.get(\"message\", \"\").strip()\n",
        "        logger.info(f\"Received chat request with input: '{user_input}'\")\n",
        "        if not user_input:\n",
        "            logger.warning(\"Received empty input message.\")\n",
        "            return jsonify({\"error\": \"メッセージを入力してください。\"}), 400 # Bad Request\n",
        "\n",
        "        # Rinna Llama 3 Youko 用のプロンプト形式\n",
        "        # https://huggingface.co/rinna/llama-3-youko-8b#%E6%8E%A8%E8%AB%96%E6%96%B9%E6%B3%95\n",
        "        prompt = (\n",
        "            \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "            f\"{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        )\n",
        "\n",
        "        # 入力をトークン化してモデルと同じデバイスに送る\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
        "        # add_special_tokens=False にしてプロンプトテンプレートの特殊トークンと重複しないように\n",
        "\n",
        "        # モデル応答生成\n",
        "        logger.info(\"Generating model response...\")\n",
        "        with torch.no_grad(): # 勾配計算を無効化\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,  # 生成トークン数を少し増やす\n",
        "                temperature=0.7,     # 温度 (低いほど決定的、高いほど多様)\n",
        "                top_p=0.9,           # Top-pサンプリング\n",
        "                do_sample=True,      # サンプリングを有効化\n",
        "                pad_token_id=tokenizer.eos_token_id, # パディングID\n",
        "                # repetition_penalty=1.1, # ★★★ 繰り返し抑制ペナルティを追加 ★★★\n",
        "                # 停止トリガーとなるトークンID\n",
        "                eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
        "            )\n",
        "        logger.info(\"Model response generated.\")\n",
        "\n",
        "        # 生成された応答全体をデコード (特殊トークンも含めて)\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "        logger.debug(f\"Raw model output: {full_response}\")\n",
        "\n",
        "        # --- 応答の後処理 ---\n",
        "        # アシスタント応答の開始位置を探す (プロンプトテンプレートに対応)\n",
        "        assistant_marker = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        # 応答は入力の後に追加されるので、rfindで最後に出現するものを探す\n",
        "        response_start_index = full_response.rfind(assistant_marker)\n",
        "\n",
        "        if response_start_index != -1:\n",
        "            # マーカーが見つかった場合、マーカー以降を抽出\n",
        "            response_text = full_response[response_start_index + len(assistant_marker):]\n",
        "        else:\n",
        "            # マーカーが見つからない場合 (フォールバック)\n",
        "            logger.warning(\"Assistant marker not found in response. Attempting to remove prompt manually.\")\n",
        "            # 入力プロンプトで始まっているか確認して除去\n",
        "            if full_response.startswith(prompt):\n",
        "                 response_text = full_response[len(prompt):]\n",
        "            else:\n",
        "                 response_text = full_response # そのまま返す (望ましくない可能性)\n",
        "\n",
        "        # 末尾の不要な特殊トークン (<|eot_id|> や <|end_of_text|>) を除去\n",
        "        response_text = response_text.replace(\"<|eot_id|>\", \"\").replace(\"<|end_of_text|>\", \"\").strip()\n",
        "        # --- 後処理ここまで ---\n",
        "\n",
        "        logger.info(f\"Processed response: '{response_text}'\")\n",
        "        # 応答をJSONで返す\n",
        "        return jsonify({\"response\": response_text})\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during chat processing: {e}\", exc_info=True)\n",
        "        # ユーザーには一般的なエラーメッセージを返す\n",
        "        return jsonify({\"error\": \"応答の生成中に内部エラーが発生しました。\"}), 500 # Internal Server Error"
      ],
      "metadata": {
        "id": "ydNdZs7mH2re"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# セル 7 内の start_ngrok_tunnel 関数を修正\n",
        "\n",
        "def start_ngrok_tunnel(port):\n",
        "    \"\"\"指定されたポートでngrokトンネルを開始する\"\"\"\n",
        "    global ngrok_tunnel\n",
        "    logger.info(\"Attempting to start ngrok tunnel...\")\n",
        "    try:\n",
        "        # 既存のトンネルがあれば切断 (コードは省略、前のまま)\n",
        "        # ... (Disconnect existing tunnel logic) ...\n",
        "        for tunnel in ngrok.get_tunnels():\n",
        "             if tunnel.config['addr'].endswith(f':{port}'):\n",
        "                 logger.warning(f\"Disconnecting existing ngrok tunnel: {tunnel.public_url}\")\n",
        "                 ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "        # 新しいトンネルを開始\n",
        "        logger.info(f\"Starting ngrok tunnel for port {port}...\")\n",
        "        ngrok_tunnel = ngrok.connect(port)\n",
        "\n",
        "        # --- ★★★ デバッグ出力追加 ★★★ ---\n",
        "        print(f\"DEBUG [ngrok.connect completed]: Tunnel object = {ngrok_tunnel}\") # connect直後のオブジェクト確認\n",
        "        if ngrok_tunnel and hasattr(ngrok_tunnel, 'public_url') and ngrok_tunnel.public_url:\n",
        "             print(f\"DEBUG https://www.merriam-webster.com/dictionary/check: Public URL = {ngrok_tunnel.public_url}\") # URLを強制表示\n",
        "             logger.info(f\"Ngrok tunnel started successfully! Public URL: {ngrok_tunnel.public_url}\") # 元のログ\n",
        "             return True\n",
        "        else:\n",
        "             # URLが取得できなかった場合\n",
        "             logger.error(\"Ngrok tunnel object obtained, BUT failed to get a valid public_url.\")\n",
        "             print(f\"DEBUG https://github.com/travis-ci/travis-ci/issues/9719: Tunnel object = {ngrok_tunnel}\")\n",
        "             return False\n",
        "        # --- ★★★ デバッグ出力ここまで ★★★ ---\n",
        "\n",
        "    except pyngrok_exception.PyngrokNgrokError as e:\n",
        "         # ... (既存のエラー処理) ...\n",
        "         logger.error(f\"Failed to start ngrok tunnel (PyngrokNgrokError): {e}\", exc_info=True)\n",
        "         if \"ERR_NGROK_4018\" in str(e) or \"authentication failed\" in str(e).lower():\n",
        "             logger.critical(\">>> Ngrok authentication failed. Check your authtoken in Colab Secrets (Step 1).\")\n",
        "         elif \"account limited\" in str(e).lower():\n",
        "             logger.error(\">>> Your ngrok account might be limited (e.g., concurrent tunnels). Check your ngrok dashboard.\")\n",
        "         return False\n",
        "    except Exception as e:\n",
        "        # ... (既存のエラー処理) ...\n",
        "        logger.error(f\"An unexpected error occurred while starting ngrok: {e}\", exc_info=True)\n",
        "        return False\n",
        "\n",
        "def shutdown_ngrok_tunnel():\n",
        "    \"\"\"ngrokトンネルを安全にシャットダウンする\"\"\"\n",
        "    global ngrok_tunnel\n",
        "    if ngrok_tunnel:\n",
        "        logger.info(f\"Disconnecting ngrok tunnel: {ngrok_tunnel.public_url}\")\n",
        "        try:\n",
        "            ngrok.disconnect(ngrok_tunnel.public_url)\n",
        "            ngrok_tunnel = None\n",
        "            logger.info(\"Ngrok tunnel disconnected successfully.\")\n",
        "        except Exception as e:\n",
        "            # 切断エラーは警告に留める\n",
        "            logger.warning(f\"Could not disconnect ngrok tunnel {ngrok_tunnel.public_url}: {e}\")\n",
        "    else:\n",
        "        logger.info(\"No active ngrok tunnel to disconnect.\")\n",
        "\n",
        "def run_flask_app(port):\n",
        "    \"\"\"Flask開発サーバーを実行する\"\"\"\n",
        "    logger.info(f\"Starting Flask development server on port {port}...\")\n",
        "    # Werkzeug (Flaskの開発サーバー) のログレベルを WARNING に設定してアクセスログを抑制\n",
        "    log = logging.getLogger('werkzeug')\n",
        "    log.setLevel(logging.WARNING)\n",
        "    # host='0.0.0.0' はコンテナ環境などで外部からアクセス可能にするためだが、\n",
        "    # ngrokを使う場合は localhost (127.0.0.1) またはデフォルトで良い\n",
        "    app.run(port=port) # debug=False が本番環境では推奨\n",
        "\n",
        "# --- メイン実行ブロック ---\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Starting main execution block...\")\n",
        "    # ngrokトンネルを開始\n",
        "    if start_ngrok_tunnel(FLASK_PORT):\n",
        "        try:\n",
        "            # ngrok起動成功後、Flaskアプリを実行\n",
        "            run_flask_app(FLASK_PORT)\n",
        "        except KeyboardInterrupt:\n",
        "            # Ctrl+C で中断した場合\n",
        "            logger.info(\"Flask app interrupted by user (KeyboardInterrupt).\")\n",
        "        except Exception as e:\n",
        "             # Flask実行中の予期せぬエラー\n",
        "             logger.error(f\"An error occurred while running the Flask app: {e}\", exc_info=True)\n",
        "        finally:\n",
        "            # Flaskアプリ終了時にngrokをシャットダウン\n",
        "            logger.info(\"Flask app finished or interrupted. Shutting down ngrok tunnel...\")\n",
        "            shutdown_ngrok_tunnel()\n",
        "            logger.info(\"Application shutdown complete.\")\n",
        "    else:\n",
        "        # ngrok起動失敗\n",
        "        logger.error(\"Failed to start ngrok tunnel. Flask app will not run.\")\n",
        "        print(\"\\n--- ngrokトンネルの起動に失敗しました ---\")\n",
        "        print(\"考えられる原因:\")\n",
        "        print(\"1. ngrok認証トークンがColabシークレットに正しく設定されていない (ステップ1を確認)。\")\n",
        "        print(\"2. ngrokアカウントの制限 (無料プランの同時接続数など)。\")\n",
        "        print(\"3. 一時的なネットワークの問題。\")\n",
        "        print(\"ログを確認し、問題を解決してから再実行してください。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4icMLz8H_mY",
        "outputId": "39f6fc48-04e7-4a7d-afe6-7452af4947a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-03 04:55:39 - __main__ - INFO - Starting main execution block...\n",
            "2025-05-03 04:55:39 - __main__ - INFO - Attempting to start ngrok tunnel...\n",
            "2025-05-03 04:55:39 - pyngrok.process - INFO - Overriding default auth token\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=start pg=/api/tunnels id=0a6f456e02bb9561\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=end pg=/api/tunnels id=0a6f456e02bb9561 status=200 dur=284.084µs\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=start pg=/api/tunnels id=812ea2a29064e28b\n",
            "2025-05-03 04:55:39 - __main__ - INFO - Starting ngrok tunnel for port 5000...\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=end pg=/api/tunnels id=812ea2a29064e28b status=200 dur=56.971µs\n",
            "2025-05-03 04:55:39 - pyngrok.ngrok - INFO - Opening tunnel named: http-5000-0f0644a8-ad35-4cc5-b00a-9c73f74b950e\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=start pg=/api/tunnels id=83e87b9a6ab09953\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=end pg=/api/tunnels id=83e87b9a6ab09953 status=200 dur=57.498µs\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=start pg=/api/tunnels id=646870439d570834\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-5000-0f0644a8-ad35-4cc5-b00a-9c73f74b950e addr=http://localhost:5000 url=https://205e-34-171-223-172.ngrok-free.app\n",
            "2025-05-03 04:55:39 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:39+0000 lvl=info msg=end pg=/api/tunnels id=646870439d570834 status=201 dur=37.542585ms\n",
            "2025-05-03 04:55:39 - __main__ - INFO - Ngrok tunnel started successfully! Public URL: https://205e-34-171-223-172.ngrok-free.app\n",
            "2025-05-03 04:55:39 - __main__ - INFO - Starting Flask development server on port 5000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG [ngrok.connect completed]: Tunnel object = NgrokTunnel: \"https://205e-34-171-223-172.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            "DEBUG https://www.merriam-webster.com/dictionary/check: Public URL = https://205e-34-171-223-172.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-03 04:55:46 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:46+0000 lvl=info msg=\"join connections\" obj=join id=2a62c735e5d7 l=127.0.0.1:5000 r=220.96.68.141:57492\n",
            "2025-05-03 04:55:47 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:55:47+0000 lvl=info msg=\"join connections\" obj=join id=a765077b97ff l=127.0.0.1:5000 r=220.96.68.141:57492\n",
            "2025-05-03 04:56:00 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:56:00+0000 lvl=info msg=\"join connections\" obj=join id=e947c8eee8f6 l=127.0.0.1:5000 r=220.96.68.141:57492\n",
            "2025-05-03 04:56:00 - __main__ - INFO - Received chat request with input: 'こんにちは'\n",
            "2025-05-03 04:56:00 - __main__ - INFO - Generating model response...\n",
            "2025-05-03 04:56:02 - __main__ - INFO - Model response generated.\n",
            "2025-05-03 04:56:02 - __main__ - INFO - Processed response: 'こんにちは、編集部の助手です。助手は、編集部の仕事を手伝ってくれる、助手です。助手は、編集部の仕事を手伝ってくれる、助手です。助手'\n",
            "2025-05-03 04:56:14 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:56:14+0000 lvl=info msg=\"join connections\" obj=join id=a9d942921095 l=127.0.0.1:5000 r=220.96.68.141:57492\n",
            "2025-05-03 04:56:14 - __main__ - INFO - Received chat request with input: '日本の首都は？'\n",
            "2025-05-03 04:56:14 - __main__ - INFO - Generating model response...\n",
            "2025-05-03 04:56:16 - __main__ - INFO - Model response generated.\n",
            "2025-05-03 04:56:16 - __main__ - INFO - Processed response: '日本の首都は？\n",
            "\n",
            "択一\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一\n",
            "\n",
            "択一'\n",
            "2025-05-03 04:56:27 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:56:27+0000 lvl=info msg=\"join connections\" obj=join id=4263aaef264e l=127.0.0.1:5000 r=220.96.68.141:57492\n",
            "2025-05-03 04:56:33 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:56:33+0000 lvl=info msg=\"join connections\" obj=join id=95469fa2318b l=127.0.0.1:5000 r=220.96.68.141:57492\n",
            "2025-05-03 04:56:33 - __main__ - INFO - Received chat request with input: 'こんにちは'\n",
            "2025-05-03 04:56:33 - __main__ - INFO - Generating model response...\n",
            "2025-05-03 04:56:36 - __main__ - INFO - Model response generated.\n",
            "2025-05-03 04:56:36 - __main__ - INFO - Processed response: 'こんにちは、\n",
            "\n",
            "ユーザー1とユーザー2は、同じメールアドレスでGmailを使用しています。\n",
            "そのため、ユーザー1がログインした状態で、ユーザー2がメ'\n",
            "2025-05-03 04:56:53 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:56:53+0000 lvl=info msg=\"join connections\" obj=join id=8ecdfa636da3 l=127.0.0.1:5000 r=220.96.68.141:57492\n",
            "2025-05-03 04:56:53 - __main__ - INFO - Received chat request with input: '日本の首都名を教えて'\n",
            "2025-05-03 04:56:53 - __main__ - INFO - Generating model response...\n",
            "2025-05-03 04:56:54 - __main__ - INFO - Model response generated.\n",
            "2025-05-03 04:56:54 - __main__ - INFO - Processed response: '日本の首都名を教えています。\n",
            "\n",
            "日本の首都は、東京です。'\n",
            "2025-05-03 04:57:16 - pyngrok.process.ngrok - INFO - t=2025-05-03T04:57:16+0000 lvl=info msg=\"join connections\" obj=join id=253f0bda733c l=127.0.0.1:5000 r=220.96.68.141:57492\n",
            "2025-05-03 04:57:16 - __main__ - INFO - Received chat request with input: '日本で一番高い山は何山？'\n",
            "2025-05-03 04:57:16 - __main__ - INFO - Generating model response...\n",
            "2025-05-03 04:57:18 - __main__ - INFO - Model response generated.\n",
            "2025-05-03 04:57:18 - __main__ - INFO - Processed response: '日本で一番高い山は何山？\n",
            "\n",
            "日本で一番高い山は何山？\n",
            "\n",
            "\n",
            "北海道の利尻山です。\n",
            "\n",
            "標高 1721メートルです。\n",
            "\n",
            "日本で一番高い山は何山？'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# MODEL_NAMEとMODEL_DIRの設定に合わせてパスを確認・修正してください\n",
        "model_cache_name = \"rinna_llama-3-youko-8b\" # MODEL_NAME.replace('/', '_') と同じはず\n",
        "cache_dir_to_delete = os.path.join(\"/content/drive/MyDrive/llama_model_cache\", model_cache_name)\n",
        "\n",
        "print(f\"Attempting to delete cache directory: {cache_dir_to_delete}\")\n",
        "if os.path.exists(cache_dir_to_delete):\n",
        "    try:\n",
        "        shutil.rmtree(cache_dir_to_delete)\n",
        "        print(f\"Successfully deleted directory: {cache_dir_to_delete}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting directory {cache_dir_to_delete}: {e}\")\n",
        "else:\n",
        "    print(f\"Cache directory not found: {cache_dir_to_delete}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M6ESEEvLb88",
        "outputId": "8ab3094d-f13d-4868-d41d-3e7591dd1792"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to delete cache directory: /content/drive/MyDrive/llama_model_cache/rinna_llama-3-youko-8b\n",
            "Successfully deleted directory: /content/drive/MyDrive/llama_model_cache/rinna_llama-3-youko-8b\n"
          ]
        }
      ]
    }
  ]
}